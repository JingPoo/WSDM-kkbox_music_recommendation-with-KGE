{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from KGE.models.translating_based.TransE import TransE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tShow the summary\t\n",
    "\t\t- KG\n",
    "\t\t    - number of hrt triplets\n",
    "\t\t    - number of entities\n",
    "\t\t    - number of entity type(in dict)\n",
    "\t\t    - number of entities group by type(in dict)\n",
    "\t\t    - number of relation \n",
    "\t\t\n",
    "\t\t- For train, validation, test各自的\n",
    "\t\t    - number of hrt triplets\n",
    "\t\t    - number of has_interest hrt triplet\n",
    "\t\t    - number of distinct user\n",
    "    \t\t- number of distinct item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_kg_summary():\n",
    "  \n",
    "    kg = pd.read_csv('./data/KKBOX/kgdata_all.csv')\n",
    "    \n",
    "    with open('./data/KKBOX/type_dict.json') as f:\n",
    "        type_dict = json.load(f)\n",
    "    \n",
    "    \n",
    "    print('Summary of KG\\n'\n",
    "          '-------------')\n",
    "\n",
    "    # - number of hrt triplets\n",
    "    print('number of hrt triplets: ', kg.shape[0])\n",
    "\n",
    "    # - number of entities\n",
    "    # make a dataframe append h & t -> calculate N_entities\n",
    "    ht_df = kg['h'].append(kg['t'],ignore_index=True)\n",
    "    print('number of entities: ', ht_df.nunique())\n",
    "\n",
    "    # - number of entity type(in dict)\n",
    "    print('number of entity type: ', len(list(set(type_dict.values()))))\n",
    "\n",
    "    # - number of entities group by type(in dict)\n",
    "    entity_groupby_type = {type:[entity for entity in type_dict.keys() if type_dict[entity] == type] for type in set(type_dict.values())}\n",
    "    N_entity_groupby_type = {type:len(entity_groupby_type[type]) for type in entity_groupby_type.keys()}\n",
    "    print('number of entities group by type: ', N_entity_groupby_type)\n",
    "\n",
    "    # - number of relations\n",
    "    print('number of relations: ', kg['r'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_summary():\n",
    "      \n",
    "      train_df = pd.read_csv('./data/KKBOX/train_data.csv')\n",
    "      valid_df = pd.read_csv('./data/KKBOX/valid_data.csv')\n",
    "      test_df = pd.read_csv('./data/KKBOX/test_data.csv')\n",
    "      \n",
    "      with open('./data/KKBOX/type_dict.json') as f:\n",
    "            type_dict = json.load(f)\n",
    "      \n",
    "      \n",
    "      print('Summary of train, validation, test data\\n'\n",
    "            '---------------------------------------')\n",
    "\n",
    "      # - number of hrt triplets\n",
    "      N_hrt_triple = {'train':len(train_df), 'validation':len(valid_df), 'test':len(test_df)}\n",
    "      print('number of hrt triplets: ', N_hrt_triple) \n",
    "      \n",
    "      # - number of has_interest hrt triplet\n",
    "      N_interest_hrt_triple = {'train':train_df[train_df['r'] == 'has_interest'].shape[0],\\\n",
    "                              'validation':valid_df[valid_df['r'] == 'has_interest'].shape[0],\\\n",
    "                              'test':test_df[test_df['r'] == 'has_interest'].shape[0]}\n",
    "      print('number of has_interest hrt triplets: ', N_interest_hrt_triple) \n",
    "      \n",
    "\n",
    "      ht_train_df = train_df['h'].append(train_df['t'],ignore_index=True)\n",
    "      type_ht_train_df = [type_dict.get(ent) for ent in ht_train_df.unique()]\n",
    "      ht_valid_df = valid_df['h'].append(valid_df['t'],ignore_index=True)\n",
    "      type_ht_valid_df = [type_dict.get(ent) for ent in ht_valid_df.unique()]\n",
    "      ht_test_df = test_df['h'].append(test_df['t'],ignore_index=True)\n",
    "      type_ht_test_df = [type_dict.get(ent) for ent in ht_test_df.unique()]\n",
    "      \n",
    "      # - number of distinct user\n",
    "      N_user = {'train':type_ht_train_df.count('member'), 'validation':type_ht_valid_df.count('member'),\\\n",
    "                'test':type_ht_test_df.count('member')}\n",
    "      print('number of distinct user: ', N_user)\n",
    "\n",
    "      # - number of distinct item\n",
    "      N_item = {'train':type_ht_train_df.count('song'), 'validation':type_ht_valid_df.count('song'),\\\n",
    "                'test':type_ht_test_df.count('song')}\n",
    "      print('number of distinct item: ', N_item)\n",
    "      \n",
    "      # # - number of distinct user\n",
    "      # # user -    5    - item\n",
    "      # # user - 0,2,3,8 - other\n",
    "      # # item - 1,4,6,7 - other\n",
    "      # N_train_user = train_df[(train_df['r'] == 0) | (train_df['r'] == 2) | (train_df['r'] == 3) |\\\n",
    "      #                         (train_df['r'] == 5) | (train_df['r'] == 8)]['h'].nunique()\n",
    "      # N_val_user = valid_df['h'].nunique()                                  \n",
    "      # N_test_user = test_df['h'].nunique() \n",
    "      # N_user = {'train':N_train_user, 'validation':N_val_user, 'test':N_test_user}\n",
    "\n",
    "      # print('number of distinct user: ', N_user)\n",
    "\n",
    "      # # - number of distinct item\n",
    "      # N_train_item = train_df[(train_df['r'] == 5)]['t'].nunique() + \\\n",
    "      #                train_df[(train_df['r'] == 1) | (train_df['r'] == 4) | (train_df['r'] == 6) | \\\n",
    "      #                         (train_df['r'] == 7)]['h'].nunique()      \n",
    "      # N_val_item = valid_df['t'].nunique()    \n",
    "      # N_test_item = test_df['t'].nunique() \n",
    "      # N_item = {'train':N_train_item, 'validation':N_val_item, 'test':N_test_item}\n",
    "\n",
    "      # print('number of distinct item: ', N_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_kg_summary()\n",
    "print()\n",
    "show_data_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/KKBOX/train_index_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f2b5b3b4e8d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/KKBOX/train_index_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/KKBOX/valid_index_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/KKBOX/test_index_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m             )\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1863\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \"\"\"\n\u001b[1;32m-> 1357\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    640\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/KKBOX/train_index_data.csv'"
     ]
    }
   ],
   "source": [
    "# read data before model training\n",
    "train = pd.read_csv('./data/KKBOX/train_index_data.csv').values\n",
    "valid = pd.read_csv('./data/KKBOX/valid_index_data.csv').values\n",
    "test = pd.read_csv('./data/KKBOX/test_index_data.csv').values\n",
    "\n",
    "with open('./data/KKBOX/metadata.json') as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized TransE model object\n",
    "model = TransE(\n",
    "    embedding_params={\"embedding_size\": 10},\n",
    "    negative_ratio=4,\n",
    "    corrupt_side=\"h+t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.train(train_X=train, val_X=valid, metadata=metadata, epochs=100, batch_size=30000,\n",
    "            early_stopping_rounds=None, restore_best_weight=False,\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "            seed=12345, log_path=\"./tensorboard_logs/eb10lr0001\", log_projector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n = 1):\n",
    "    # generate batches of batch_size:n \n",
    "    current_batch = []\n",
    "    for item in iterable:\n",
    "        current_batch.append(item)\n",
    "        if len(current_batch) == n:\n",
    "            yield current_batch\n",
    "            current_batch = []\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "        \n",
    "def recommend(user_list):\n",
    "    '''\n",
    "    A function to recommend 25 musics for each user in the input user list\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "            user_list: list of user id\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "            dict: top 25 recommend songs for list of users\n",
    "    '''\n",
    "    # - input: list of user id\n",
    "    # - output: list of recommend item (25 recommend songs for each user)\n",
    "    # - logic:\n",
    "    #     1. user id → user embedding\n",
    "    #     2. a = user embedding + has_insterest embedding\n",
    "    #     3. compare distance with all item embeddings, output the nearest 25 items\n",
    "\n",
    "    test_users_rec_music = {}\n",
    "    for users in batch(user_list,100):\n",
    "        # users embedding (batch_users * embedding_size)\n",
    "        users_index = [metadata['ent2ind'].get(user) for user in users]\n",
    "        users_emb = tf.nn.embedding_lookup(model.model_weights['ent_emb'], users_index)\n",
    "\n",
    "        # has_interest embedding (1 * embedding_size )\n",
    "        has_interest_index = metadata['rel2ind']['has_interest']\n",
    "        has_interest_emb = model.model_weights['rel_emb'][has_interest_index]\n",
    "        \n",
    "        # compute recommend songs (batch_users * embedding_size)\n",
    "        compute_songs_emb = users_emb + has_interest_emb\n",
    "\n",
    "        with open('./data/KKBOX/entity_groupby_type.json') as f:\n",
    "            entity_groupby_type = json.load(f)\n",
    "\n",
    "        # songs embedding (total_songs * embedding_size)\n",
    "        song_id = [metadata['ent2ind'].get(ent) for ent in entity_groupby_type['song']]\n",
    "        songs_emb = tf.nn.embedding_lookup(model.model_weights['ent_emb'], song_id)\n",
    "\n",
    "        # 用matrix計算，算完全部compute_songs_emb (list) 與 全部songs_emb(list)的距離 (batch_users * total_songs)\n",
    "        distances = [] \n",
    "        # for each user\n",
    "        for i in range(compute_songs_emb.shape[0]):\n",
    "            # calculate his rec_music embedding distance to all songs embeddings\n",
    "            distances.append(tf.norm(tf.subtract(songs_emb, compute_songs_emb[i]), ord=2, axis=1))\n",
    "\n",
    "        # 每個人的前25首embedding相似的song index (batch_users * 25)\n",
    "        top_25_songs_index = tf.argsort(distances)[:,:25].numpy().tolist() \n",
    "\n",
    "        # song index to song id (batch_users * 25)\n",
    "        song_ent = tf.convert_to_tensor(np.array(entity_groupby_type['song']))\n",
    "        top_25_songs = tf.nn.embedding_lookup(song_ent, top_25_songs_index)\n",
    "\n",
    "        # zip users and their rec_25_songs into a dict\n",
    "        users_top25_songs =  dict(zip(users,top_25_songs))\n",
    "        test_users_rec_music.update(users_top25_songs)\n",
    "    \n",
    "    return test_users_rec_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG\n",
    "\n",
    "def DCG(rec_list, ans_list):\n",
    "    dcg = 0\n",
    "    for i in range(len(rec_list)):\n",
    "        r_i = 0\n",
    "        if rec_list[i] in ans_list:\n",
    "            r_i = 1\n",
    "        dcg += (2**r_i - 1) / np.log2((i + 1) + 1)\n",
    "    return dcg\n",
    "\n",
    "def IDCG(rec_list, ans_list):\n",
    "    A_temp_1 = []\n",
    "    A_temp_0 = []\n",
    "    for rec_music in rec_list:\n",
    "        if rec_music in ans_list:\n",
    "            A_temp_1.append(rec_music)\n",
    "        else:\n",
    "            A_temp_0.append(rec_music)\n",
    "    A_temp_1.extend(A_temp_0)\n",
    "    idcg = DCG(A_temp_1, ans_list)\n",
    "    return idcg\n",
    "\n",
    "def NDCG(rec_list, ans_list):\n",
    "    dcg = DCG(rec_list, ans_list)\n",
    "    idcg = IDCG(rec_list, ans_list)\n",
    "    if dcg == 0 or idcg ==0:\n",
    "        ndcg = 0\n",
    "    else:\n",
    "        ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "    \n",
    "def intersection(list1, list2):\n",
    "    # check if two lists have intersect\n",
    "    return list(set(list1) & set(list2))\n",
    "    \n",
    "def evaluate(test_users_rec_music):\n",
    "    '''\n",
    "    Evaluate the recommend result\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            test_users_rec_music(dict): top 25 recommended songs for each user\n",
    "            log_path: the path to write in tensorboard log\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            metric_result(dict): metric include hit, recall, precision and NDCG\n",
    "    '''\n",
    "    TP_list = [] # each user's True Positive number\n",
    "    ans_lengths = [] # each user's has_interest music number\n",
    "    ndcg_list = []\n",
    "    for user in test_users_rec_music.keys():\n",
    "        ans_music_list = user_and_hasInterestItem[user]\n",
    "        ans_lengths.append(len(ans_music_list))\n",
    "        rec_music_list = [x.decode() for x in test_users_rec_music[user].numpy().tolist()]\n",
    "        TP_list.append(len(intersection(rec_music_list, ans_music_list)))\n",
    "        ndcg_list.append(NDCG(rec_music_list, ans_music_list))\n",
    "        \n",
    "    hit_list = [1 if TP >= 1 else 0 for TP in TP_list]\n",
    "    precision_list = [TP/25 for TP in TP_list]\n",
    "    recall_list = [TP_list[i]/ans_lengths[i] for i in range(len(TP_list))]\n",
    "   \n",
    "    # hit_list = []\n",
    "    # recall_list = []\n",
    "    # precision_list = []\n",
    "    # for user in users:\n",
    "    #     hit_count = 0\n",
    "    #     # true answer in test\n",
    "    #     ans_music_list = user_and_hasInterestItem[user]\n",
    "        \n",
    "    #     for rec_music in rec_music_list[i]:\n",
    "    #         # check if recommended music hits\n",
    "    #         if (rec_music.decode('utf8') in ans):\n",
    "    #             hit_count += 1\n",
    "    #     # hit (是否有推薦命中)\n",
    "    #     hit_list.append(min(hit_count,1))\n",
    "    #     # recall (真實有興趣的音樂 分之 推薦命中數)\n",
    "    #     recall_list.append(hit_count/len(ans))\n",
    "    #     # precision (25個推薦音樂 分之 推薦命中數)\n",
    "    #     precision_list.append(hit_count/25)\n",
    "\n",
    "    metric_result = {\n",
    "        'hit': statistics.mean(hit_list),\n",
    "        'recall': statistics.mean(recall_list),\n",
    "        'precision': statistics.mean(precision_list),\n",
    "        'ndcg': statistics.mean(ndcg_list)\n",
    "    }\n",
    "\n",
    "    # # write in tensorboard log\n",
    "    # summary_writer = tf.summary.create_file_writer(log_path)\n",
    "    # with summary_writer.as_default():\n",
    "    #     tf.summary.scalar('hit', evaluate_result['hit'], step=0)\n",
    "    #     tf.summary.scalar('recall', evaluate_result['recall'], step=0)\n",
    "    #     tf.summary.scalar('precision', evaluate_result['precision'], step=0)\n",
    "    #     tf.summary.scalar('ndcg', evaluate_result['ndcg'], step=0)\n",
    "\n",
    "    return metric_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data\n",
    "test_data = pd.read_csv('./data/KKBOX/test_data.csv')\n",
    "test_users = test_data['h'].unique().tolist()\n",
    "user_and_hasInterestItem = test_data.groupby('h')['t'].apply(list).to_dict()\n",
    "\n",
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend and evaluate on TEST data\n",
    "\n",
    "test_users_rec_music = recommend(test_users)\n",
    "test_evaluate_result = evaluate(test_users_rec_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write in tensorboard log\n",
    "summary_writer = tf.summary.create_file_writer('./tensorboard_logs/eb10lr0001')\n",
    "with summary_writer.as_default():\n",
    "    tf.summary.scalar('test-hit', test_evaluate_result['hit'], step=0)\n",
    "    tf.summary.scalar('test-recall', test_evaluate_result['recall'], step=0)\n",
    "    tf.summary.scalar('test-precision', test_evaluate_result['precision'], step=0)\n",
    "    tf.summary.scalar('test-ndcg', test_evaluate_result['ndcg'], step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend and evaluate an TRAINING data\n",
    "train_data = pd.read_csv('./data/KKBOX/train_data.csv')\n",
    "train_data = train_data[train_data['r']=='has_interest']\n",
    "train_users = train_data['h'].unique().tolist()\n",
    "user_and_hasInterestItem = train_data.groupby('h')['t'].apply(list).to_dict()\n",
    "\n",
    "train_users_rec_music = recommend(train_users)\n",
    "train_evaluate_result = evaluate(train_users_rec_music)\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer('./tensorboard_logs/eb10lr0001')\n",
    "with summary_writer.as_default():\n",
    "    tf.summary.scalar('train-hit', train_evaluate_result['hit'], step=0)\n",
    "    tf.summary.scalar('train-recall', train_evaluate_result['recall'], step=0)\n",
    "    tf.summary.scalar('train-precision', train_evaluate_result['precision'], step=0)\n",
    "    tf.summary.scalar('train-ndcg', train_evaluate_result['ndcg'], step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend and evaluate an VALIDATION data\n",
    "valid_data = pd.read_csv('./data/KKBOX/valid_data.csv')\n",
    "valid_users = valid_data['h'].unique().tolist()\n",
    "user_and_hasInterestItem = valid_data.groupby('h')['t'].apply(list).to_dict()\n",
    "\n",
    "valid_users_rec_music = recommend(valid_users)\n",
    "valid_evaluate_result = evaluate(valid_users_rec_music)\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer('./tensorboard_logs/eb10lr0001')\n",
    "with summary_writer.as_default():\n",
    "    tf.summary.scalar('valid-hit', valid_evaluate_result['hit'], step=0)\n",
    "    tf.summary.scalar('valid-recall', valid_evaluate_result['recall'], step=0)\n",
    "    tf.summary.scalar('valid-precision', valid_evaluate_result['precision'], step=0)\n",
    "    tf.summary.scalar('valid-ndcg', valid_evaluate_result['ndcg'], step=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41df773a823dbf9f934078d72fc2940838300d32d879c38d557790ddf6e50e9d"
  },
  "kernelspec": {
   "display_name": "Python 3.5.5 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
